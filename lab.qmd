---
title: "In-Class Lab Exercises"
subtitle: "MATH/COSC 3570 Spring 2025"
date: today
author: "Luke Syverud"
format:
  html:
    toc: true
execute:
  echo: true
  eval: false
---

```{r}
#| label: setup
#| include: false
library(reticulate)
library(tidyverse)
#library(tidymodels)
```



## Lab 1: Running R Script

```{r}
#| label: lab-1
#| eval: true
library(ggplot2)
bar <- ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut), 
           show.legend = FALSE, width = 1) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)
bar + coord_flip()
bar + coord_polar()
```

We've got this!❤️



## Lab 2: Quarto
We can use code-fold to collapse the code into an HTML <detail> tag. We can include "code-fold:true" underneath the "toc: true" block. This way, all the code won't automatically display, but can be accessed by the user by clicking the tag.

Briefly describe how we produce a pdf.

To produce a pdf, we can change the format in the YAML header to pdf. However, to create the pdf, we need to install a recent distribution of TeX, and then the changing of the format will work.

<!-- Once done, commit with message "02-quarto" and push your version to GitHub. -->


## Lab 3: Markdown

Add a self-introduction paragraph containing a header, bold and italic text.

### Luke Syverud

Hello everyone! I am a Junior at Marquette and I am studing **Math** and **Political Science**. I am super interested in *current events* and *data analysis*, but in my free time I really enjoy *gaming*, *reading*, and *hanging out with friends*.

Add another paragraph that contains listed items, a hyperlink, a blockquote, and math expression.

* Players on the Milwaukee Bucks:
  + Giannis Antetokounmpo
  + Damian Lillard
  + Khris Middleton
  + Bobby Portis
  + etc.

[Bucks Roster](https://www.nba.com/bucks/roster)

>The Pythagorean Theorem

$$a^{2} + b^{2} = c^{2}$$

<!-- Once done, commit with message "03-markdown" and push your updated work to GitHub. -->


## Lab 4: Code Chunk

```{r}
#| eval: true
#| fig-width: 4
#| fig-height: 2
# include image with knitr::include_graphics()
knitr::include_graphics("https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/ggplot2.png")
```

```{r}
#| eval: true
#| fig-height: 4
#| fig-width: 6
#| fig-align: right
# include plot using plot(mtcars$disp, mtcars$mpg)
plot(mtcars$disp, mtcars$mpg)
```

```{r}
#| eval: true
# show dataset `mtcars` using knitr::kable()
knitr::kable(mtcars)
```

Inline Calculations:
`r ncol(mtcars)`, `r log(100, base = 10) + sqrt(4)`

Add option fig-height: 4, fig-width: 6 and fig-align: right to your plot.

How do we set global chunk options to hide and run code in every chunk?

We can specify the global echo as "false" in the execute options at the start of the document.

<!-- Once done, commit with message "04-codechunk" and push your work to GitHub. -->


## Lab 5: R Data Type Summary

- Create R objects vector `v1`, factor `f2`, list `l3`, matrix `m4` and data frame `d5`.

- Check `typeof()` and `class()` of those objects, and create a list having the output below.

```{r}
#| eval: true
v1 <- c(1,2,3)
f2 <- factor("first", "second", "last")
l3 <- list("one", "two", "three")
m4 <- matrix(data = 1:4, nrow = 2, ncol = 2)
d5 <- data.frame(age = c(20,21,22), 
                 name = c("Luke", "Mia", "Emma"))

v <- c(type = typeof(v1), class = class(v1))
f <- c(type = typeof(f2), class = class(f2))
l <- c(type = typeof(l3), class = class(l3))
m <- c(type = typeof(m4), class = class(m4))
d  <- c(type = typeof(d5), class = class(d5))

list(vec   =   v,
     fac = f,
     lst = l,
     mat = m,
     df = d)
```

<!-- Once done, commit with message "05-rdata" and push your work to GitHub. -->

## Lab 6: Python Data Structure

```{r}
x_lst <- list(idx = 1:3, 
              word = "a", 
              bool = c(TRUE, FALSE))
```

```{python}
#| echo: true
#| eval: true
#| code-line-numbers: false

py_list = [[1,2,3], "a", [True,False]]
py_list
py_dic = {'list': [1,2,3], "letter": "a", "Bool": [True,False]}
py_dic
```


<!-- Once done, commit with message "06-pythondata" and push your work to GitHub. -->





## Lab 7: Plotting

For the `mtcars` data, use R and Python to 

  + make a scatter plot of miles per gallon vs. weight. Decorate your plot using arguments, `col`, `pch`, `xlab`, etc. Check `?par` for more graphical parameters.
  
  + create a histogram of 1/4 mile time. Make it beautiful!
  
  + generate boxplots of miles per gallon by number of forward gears. Make it cool!


```{python}
import pandas as pd
import matplotlib.pyplot as plt
mtcars = pd.read_csv('./data/mtcars.csv')
```

```{r}
#| eval: true
plot(x = mtcars$mpg, y = mtcars$wt,
     xlab = "Miles Per Gallon",
     ylab = "Weight",
     main = "Scatter Plot",
     col = "green",
     pch = 12)
```

```{r}
#| eval: true

hist(mtcars$qsec,
     breaks = 9,
     col = "green",
     border = "yellow",
     xlab = "1/4 Mile Time",
     ylab = "Frequency",
     main = "Histogram of 1/4 Mile Time")

```

```{r}
#| eval: true

boxplot(mpg ~ gear,
        data = mtcars,
        xlab = "Miles Per Gallon",
        ylab = "Forward Gears",
        col = c("blue", "green", "yellow"),
        horizontal = TRUE)
```

```{python}
#| eval: true
import pandas as pd
import matplotlib.pyplot as plt
mtcars = pd.read_csv('./data/mtcars.csv')
plt.scatter(x = mtcars.mpg,
            y = mtcars.wt,
            color = "r")
plt.xlabel("Miles per gallon")
plt.ylabel("Weight")
plt.title("Scatter Plot")
```

```{python}
#| eval: true
import pandas as pd
import matplotlib.pyplot as plt
mtcars = pd.read_csv('./data/mtcars.csv')

plt.hist(mtcars.qsec,
bins = 9,
color = "g",
edgecolor = "y"
)
plt.xlabel("1/4 Mile Time")
plt.title("Histogram of 1/4 Mile Time")
```

```{python}
#| eval: true
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
mtcars = pd.read_csv('./data/mtcars.csv')

gear_num = np.unique(mtcars.gear)
gear_list = []
gear_list.append(mtcars[mtcars.gear == gear_num[0]].mpg)
gear_list.append(mtcars[mtcars.gear == gear_num[1]].mpg)
gear_list.append(mtcars[mtcars.gear == gear_num[2]].mpg)

plt.boxplot(gear_list, vert = False, tick_labels = [3,4,5])
plt.xlabel("Miles Per Gallon")
plt.ylabel("Frequency")
plt.ylabel("Forward Gears")

```

<!-- Once done, commit with message "07-plotting" and push your work to GitHub. -->

## Lab 8: Tibbles and Pipes

```{r}
#| eval: true
df <- data.frame(abc = 1:2, 
                 xyz = c("a", "b"))
# list method
df$x
df[[2]]
df["xyz"]
df[c("abc", "xyz")]
# matrix method
df[, 2]
df[, "xyz"]
df[, c("abc", "xyz")]
```

```{r}
#| eval: true

library(tidyverse)
tib <- tibble(abc = 1:2, 
              xyz = c("a", "b"))
# list method
tib$x
tib[[2]]
tib["xyz"]
tib[c("abc", "xyz")]
# matrix method
tib[, 2]
tib[, "xyz"]
tib[, c("abc", "xyz")]
```

Explain their differences.

The "df$x" operation for data frames access the "xyz" column through partial name matching. This does not occur for the tibble because it doesn't do partial name matching.

The df[[2]] and tib[[2]] operations produce the same output because they index in the same way.

df["xyz"] produces a data frame, while tib["xyz"] produces another tibble.

df[c("abc", "xyz")] produces a data frame with both columns, while tib[c("abc", "xyz")] produces a tibble showing both columns and their types.

df[, 2] subsets df and produces a data frame. tib[, 2] produces a tibble.

df[, "xyz"] produces a data frame, tib[, "xyz"] produces a tibble

df[, c("abc", "xyz")] produces a data frame. tib[, c("abc", "xyz")] produces a tibble.

```{r}
#| eval: true
# pipes for iris summary
iris %>% tail(n = 12)
summary(iris)
```


<!-- Once done, commit with message "08-tibble" and push your work to GitHub. -->





## Lab 9: NumPy and pandas

```{r}
#| eval: true
tibble(x = 1:5, y = 5:1, z = LETTERS[1:5])
```

```{python}
#| eval: true
# creating a Python pandas.DataFrame equivalent to the R tibble

import numpy as np
import pandas as pd
import string as st
dic = {'x': np.arange(1,6), 
       'y': reversed([1,2,3,4,5]),
       'z': list(st.ascii_uppercase)[0:5]}
df= pd.DataFrame(dic)
df.index = [1,2,3,4,5]
print(df)
```

<!-- Once done, commit with message "09-numpy" and push your work to GitHub. -->

```{r}
#| eval: true

x <- seq(0, 2*pi, by = 0.01)
xhrt <- 16 * sin(x) ^ 3
yhrt <- 13 * cos(x) - 5 * cos(2*x) - 2 * cos(3*x) - cos(4*x)
par(mar = c(0, 0, 0, 0))
plot(xhrt, yhrt, type = "l", axes = FALSE, xlab = "", ylab = "")
polygon(xhrt, yhrt, col = "red", border = NA)
points(c(10,-10, -15, 15), c(-10, -10, 10, 10), pch = 169, font = 5)
text(0, 0, "Happy Valentine's Day!", font = 2, cex = 2, col = "pink")
```


## Lab 10: Import Data

```{r}
#| eval: true
ssa_male <- readr::read_csv(file = "./data/ssa_male_prob.csv", show_col_types = FALSE)
ssa_female <- readr::read_rds(file = "./data/ssa_female_prob.Rds")

plot(x = ssa_female$Age, y = ssa_female$LifeExp , type = "line", col = "red",
     xlab = "Age", ylab = "LifeExp", main = "Age vs Life Expectancy Male (Blue) and Female (Red)")
lines(x = ssa_male$Age, y = ssa_male$LifeExp, col = "blue")
```


<!-- Once done, commit with message "10-import" and push your work to GitHub. -->


## Lab 11: ggplot2

```{r}
#| eval: true
penguins <- read_csv(file = "./data/penguins.csv", show_col_types = FALSE)
penguins%>%
ggplot(mapping = aes(x = bill_depth_mm,
                        y = bill_length_mm,
                        colour = species)) +
  geom_point() +
  labs(title = "Bill Depth and Length",
       subtitle = "Dimensions for Adelie, Chinstrap, and Gentoo Penguins",
       x = "Bill Depth (mm)", y = "Bill Length (mm)",
       color = "Species",
       caption = "Source: Palmer Station LTER / palmerpenguins package")+
  scale_color_viridis_d()
```

<!-- Once done, commit with message "11-ggplot2" and push your work to GitHub. -->



## Lab 12: Faceting

```{r}
#| eval: true
ggplot(data = mpg, 
       mapping = aes(x = displ, y = cty, color = drv, shape = fl)) +
    geom_point(size = 3, alpha = 0.8) + 
    facet_grid(drv ~ fl) +
    guides(color = "none")
```


<!-- Once done, commit with message "12-faceting" and push your work to GitHub. -->


## Lab 13: Visualization

Import the data `penguins.csv`.

```{r}
read_csv("./data/penguins.csv")
```


```{r}
#| eval: true
## fig 1
library(tidyverse)
penguins <- read_csv("./data/penguins.csv", show_col_types = FALSE)
penguins |> ggplot(aes(x = species, fill = species)) +  ## mapping layer  
    geom_bar() +  ## geometry layer
    labs(x = "Species of Penguins", y = "count", title = "Species Counts in Penguins Data")  ## label layer
```

```{r}
#| eval: true
## fig 2

penguins |> ggplot(aes(x = bill_length_mm, fill = species)) +  ## mapping layer  
    geom_histogram() +  ## geometry layer
    labs(x = "Bill Length (mm)", y = "Frequency", title = "Penguins Bill Length By Species") +  ## label layer
    facet_wrap(~ species, ncol = 3)  +   ## facet layer
    theme(legend.position = "none")      ## theme layer (set legend.position = "none")
```


<!-- Once done, commit with message "13-visualization" and push your work to GitHub. -->





## Lab 14: plotly

Load tidyverse and plotly and the `loans.csv` data.

```{r}
#| eval: true
library(tidyverse)
library(plotly)
```


```{r}
#| eval: true
loans <- read_csv("./data/loans.csv",show_col_types = FALSE)
p <- plot_ly(loans, x = ~loan_amount, y = ~homeownership, alpha = 0.6, color = ~homeownership, type = "box")
p
```

<!-- Once done, commit with message "14-plotly" and push your work to GitHub. -->



## Lab 15: dplyr

```{r}
#| eval: true
library(tidyverse)
murders <- read_csv("./data/murders.csv")
# other code
my_states <- murders  |>  
    mutate(rate = total / population * 100000) |> 
    filter((region == "West" | region == "Northeast") & rate < 1) |> 
    select(state, region, rate)
my_states

my_states |> 
    group_by(region) |> 
    summarize(avg = mean(rate), stdev = sd(rate)) |> 
    arrange(-avg)
```


<!-- Once done, commit with message "15-dplyr" and push your work to GitHub. -->



## Lab 16: Joining Tables

```{r}
#| eval: true
# Import the data at https://www.jaredlander.com/data/DiamondColors.csv.
diamond_color <- readr::read_csv("https://www.jaredlander.com/data/DiamondColors.csv")

joined_df <- diamonds |>  
    left_join(diamond_color, by = c('color' = 'Color')) |> 
    select(carat, color, Description, Details)

joined_df
# Create a bar chart of the variable color
joined_df |> ggplot(aes(x = color, fill = color)) +
  geom_bar() +
  labs(x = "Color", y = "Count", title = "Color Amounts")

```


<!-- Once done, commit with message "16-joining" and push your work to GitHub. -->



## Lab 17: tidyr

```{r}
#| eval: true
# Import trump.csv
library(tidyverse)
trump_data <- read_csv("./data/trump.csv")
trump_data
```

```{r}
#| eval: true
trump_longer <- trump_data |>
    pivot_longer(
        cols = c('approval', 'disapproval'),
        names_to = "rating_type",
        values_to = "rating_value"
    ) 
trump_longer
```

```{r}
trump_longer |> ggplot(mapping= aes(x = date, y = rating_value)) +
  geom_line()+
  facet_wrap(~subgroup)+
  guides(color = "none")
# Use `trump_longer` to generate the plot below. Bonus question!
```


<!-- Once done, commit with message "17-tidyr" and push your work to GitHub. -->



## Lab 18: Probability

```{r}
#| eval: true
binomial = dbinom(x = 0:5, size = 5, prob = .3)
Prob_table= tibble(x = 0:5, y = binomial)

```

```{r}
#| eval: true
# plot the probability function
# geom_col()
Prob_table |> 
  ggplot() + 
  geom_col(aes(x = 0:5, y = binomial), width = 0.2, fill = "black") + 
  labs(x = "Number of Successes", y = "Pr(X=x)", title = "binomial(5,0.3)") + theme_minimal()

```


<!-- Once done, commit with message "18-probability" and push your work to GitHub. -->



## Lab 19: Confidence Interval

```{r}
#| eval: true

mu <- 120; sig <- 5 
al <- 0.05; M <- 100; n <- 16

set.seed(10132023)
x_rep <- replicate(M, rnorm(n, mu, sig))
xbar_rep <- apply(x_rep, 2, mean)
E <- qnorm(p = 1 - al / 2) * sig / sqrt(n)
ci_lwr <- xbar_rep - E
ci_upr <- xbar_rep + E

plot(NULL, xlim = range(c(ci_lwr, ci_upr)), ylim = c(0, 100), 
     xlab = "95% CI", ylab = "Sample", las = 1)
mu_out <- (mu < ci_lwr | mu > ci_upr)
segments(x0 = ci_lwr, y0 = 1:M, x1 = ci_upr, col = "navy", lwd = 2)
segments(x0 = ci_lwr[mu_out], y0 = (1:M)[mu_out], x1 = ci_upr[mu_out], col = 2, lwd = 2)
abline(v = mu, col = "#FFCC00", lwd = 2)

```
According to the graph above, there are 5 Confidence Intervals that do not cover the true mean as labeled in red.


<!-- Once done, commit with message "19-confidenceinterval" and push your work to GitHub. -->


## Lab 20: Linear Regression


```{r}
#| eval: true
# add the layer geom_smooth(method = "lm", se = FALSE)
library(ggthemes)
mpg|>
  ggplot(mapping = aes(x = cty, y=hwy)) +
  geom_point()+
  geom_smooth(method = "lm", se = FALSE, color = "#00008B")+
  labs(x = "City MPG", y = "Highway MPG", title = "Highway MPG vs City MPG")+
  theme_bw()
```


<!-- Once done, commit with message "20-linearregression" and push your work to GitHub. -->





## Lab 21: Logistic Regression


```{r}
#| eval: true

library(tidymodels)
bodydata <- read_csv("./data/body.csv")
body <- bodydata |> 
    select(GENDER, HEIGHT) |> 
    mutate(GENDER = as.factor(GENDER))
body |> slice(1:4)
# Fit the logistic regression
logis_out <- logistic_reg() |> 
    fit(GENDER ~ HEIGHT, 
        data = body, 
        family = "binomial")


predict(logis_out$fit, newdata = data.frame(HEIGHT = 185.42), 
        type = "response")
```


<!-- Once done, commit with message "21-logisticregression" and push your work to GitHub. -->



## Lab 22: K-Nearest Neighbors


```{r}
#| eval: true
library(tidymodels)

## load data
bodydata <- read_csv("./data/body.csv")
body <- bodydata |> 
    select(GENDER, HEIGHT, WAIST) |> 
    mutate(GENDER = as.factor(GENDER))

## training and test data
set.seed(2025)
df_split <- initial_split(data = body, prop = 0.8)
df_trn <- training(df_split)
df_tst <- testing(df_split)

## KNN training
knn_recipe <- recipe(GENDER ~ HEIGHT + WAIST, data = df_trn) |> 
    step_normalize(all_predictors())
knn_mdl <- nearest_neighbor(neighbors = 3, mode = "classification")
knn_out <- workflow() |> 
    add_recipe(knn_recipe) |> 
    add_model(knn_mdl) |> 
    fit(data = df_trn)

## KNN prediction
knn_pred <- pull(predict(knn_out, df_tst))
table(knn_pred, df_tst$GENDER)
mean(knn_pred == df_tst$GENDER)
```

```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

## load data
body = pd.read_csv('./data/body.csv')
X = body[['HEIGHT', 'WAIST']]
y = body['GENDER']

## training and test data
X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2024)

## KNN training
knn = KNeighborsClassifier(n_neighbors = 3)
X_trn = np.array(X_trn)
X_tst = np.array(X_tst)
knn.fit(X_trn, y_trn)

## KNN prediction
y_pred = knn.predict(X_tst)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_tst, y_pred)
np.mean(y_tst == y_pred)
```
**Answer:** Using more predictors increases the accuracy because it represents more of the dataset instead of a small subset. It is limited in how much more accurate it is, for example K=3 vs K=10 only differed by about 3.5% in accuracy, but it still showcases a more accurate model.
<!-- Once done, commit with message "22-knn" and push your work to GitHub. -->



## Lab 23: Principal Component Analysis


```{r}
#| eval: true

iris|> slice(1:6)

pca_output <- prcomp(iris[, -5], scale=TRUE)
(pca_output$rotation <- pca_output$rotation)
pca_output$x |> head(2) |> round(2)
summary(pca_output)
biplot(pca_output, xlabs = rep("*", nrow(iris)),
       scale = 0)
```
**This biplot showcases the PC1 vs PC2 for each datapoint in the iris dataset. It showcases the trend for which category ecah variable falls under, either PC1, making it a core component, or PC2 making it not a core component in determining the species. Petal length and Petal weidth are both shown as having a positive PC1 and near 0 PC2, meaning they both contribute greatly to determining what the species is. Sepal width and Sepal length have increased PC2, showcasing that they are not as important in determining the species.**
```{r}
#| eval: true
pca_out = pca_output
data.frame(pca_out$x[,1:2], Species = iris$Species) |>
    ggplot(aes(PC1, PC2, fill = Species)) +
    geom_point(cex = 5, pch = 21)
```

<!-- Once done, commit with message "23-pca" and push your work to GitHub. -->




## Lab 24: K-Means Clustering

```{r}
#| eval: true
library(palmerpenguins)
penguins
peng <- penguins[complete.cases(penguins), ] |> 
    select(bill_length_mm, flipper_length_mm)
```

```{r}
#| eval: true
library(cluster)

df_clust <- as_tibble(scale(peng))

kmeans = kmeans(df_clust, centers = 3, nstart = 20)
kmeans


df_clust_k <- augment(kmeans, df_clust)

tidy_kclust <- tidy(kmeans)


df_clust_k |>  
    ggplot(aes(x = bill_length_mm, 
               y = flipper_length_mm)) + 
    geom_point(aes(color = .cluster, shape = .cluster), 
               alpha = 0.8) + 
    geom_point(data = tidy_kclust |>  
                   select(1:2),
               size = 8,
               fill = "black",
               shape = "o") +
    theme_minimal() +
    theme(legend.position = "bottom")
library(factoextra)
fviz_cluster(object = kmeans, data = df_clust, label = NA) + 
    theme_bw()
```

<!-- Once done, commit with message "24-kmeans" and push your work to GitHub. -->




